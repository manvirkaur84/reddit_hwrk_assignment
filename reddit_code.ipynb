{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvrTG5eV0oDIPZnIi0t1ip"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"xEWDAo_6z-U4","executionInfo":{"status":"error","timestamp":1762107886378,"user_tz":480,"elapsed":22,"user":{"displayName":"Manvir Kaur","userId":"08029073424100200767"}},"outputId":"9d9e8e81-3363-4bf7-8e9e-ab0317b794ac"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Dict' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3024226706.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_row_from_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubreddit_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_query\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mauthor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthor\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselftext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselftext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"]}],"source":["FIELDNAMES = [\n","    \"title\",          # String\n","    \"score\",          # Integer\n","    \"upvote_ratio\",   # Float\n","    \"num_comments\",   # Integer\n","    \"author\",         # String\n","    \"subreddit\",      # String\n","    \"url\",            # String\n","    \"permalink\",      # String\n","    \"created_utc\",    # Integer\n","    \"is_self\",        # Boolean\n","    \"selftext\",       # String (truncated to 500)\n","    \"flair\",          # String\n","    \"domain\",         # String\n","    \"search_query\",   # String (None if not using search)\n","]\n","\n","def _to_int(x):\n","    try:\n","        return int(x)\n","    except Exception:\n","        return None\n","\n","def _row_from_post(post, subreddit_name: str, search_query: str | None) -> Dict[str, Any]:\n","    author = post.author.name if post.author else None\n","    text = post.selftext if isinstance(post.selftext, str) else None\n","    if text and len(text) > 500:\n","        text = text[:500] + \"...\"\n","    return {\n","        \"title\": getattr(post, \"title\", None),\n","        \"score\": getattr(post, \"score\", None),\n","        \"upvote_ratio\": getattr(post, \"upvote_ratio\", None),\n","        \"num_comments\": getattr(post, \"num_comments\", None),\n","        \"author\": author,\n","        \"subreddit\": subreddit_name,\n","        \"url\": getattr(post, \"url\", None),\n","        \"permalink\": (\"https://reddit.com\" + getattr(post, \"permalink\",\"\")) if getattr(post,\"permalink\",None) else None,\n","        \"created_utc\": _to_int(getattr(post, \"created_utc\", None)),\n","        \"is_self\": getattr(post, \"is_self\", None),\n","        \"selftext\": text,\n","        \"flair\": getattr(post, \"link_flair_text\", None),\n","        \"domain\": getattr(post, \"domain\", None),\n","        \"search_query\": search_query if search_query else None,\n","    }\n","\n","def _normalize_subs(subreddit_name) -> List[str]:\n","    if isinstance(subreddit_name, str):\n","        subs = [subreddit_name]\n","    else:\n","        subs = subreddit_name\n","    return [s.strip().lstrip(\"r/\") for s in subs if isinstance(s, str) and s.strip()]\n","\n","# ---- Task 1: HOT posts\n","def fetch_hot_posts(subreddit_name, limit=50) -> List[Dict[str,Any]]:\n","    subs = _normalize_subs(subreddit_name)\n","    rows: List[Dict[str,Any]] = []\n","    for sub in subs:\n","        print(f\"‚¨áÔ∏è  r/{sub}: collecting HOT (limit={limit}) ...\")\n","        count = 0\n","        for post in reddit.subreddit(sub).hot(limit=limit):\n","            rows.append(_row_from_post(post, sub, search_query=None))\n","            count += 1\n","        print(f\"‚úÖ Collected {count} hot posts from r/{sub}.\")\n","    return rows\n","\n","# ---- Task 2: keyword-based search\n","def search_posts(query: str, subreddit_name, limit=50) -> List[Dict[str,Any]]:\n","    subs = _normalize_subs(subreddit_name)\n","    rows: List[Dict[str,Any]] = []\n","    for sub in subs:\n","        print(f\"üîé r/{sub}: searching '{query}' (limit={limit}) ...\")\n","        count = 0\n","        for post in reddit.subreddit(sub).search(query=query, sort=\"new\", limit=limit):\n","            rows.append(_row_from_post(post, sub, search_query=query))\n","            count += 1\n","        print(f\"‚úÖ Collected {count} search posts from r/{sub}.\")\n","    return rows\n","\n","# ---- Task 3: data export to CSV\n","def export_posts_to_csv(rows: List[Dict[str,Any]], out_path: str = \"reddit_data.csv\") -> bool:\n","    if not rows:\n","        print(\"‚ö†Ô∏è No rows to export.\")\n","        return False\n","    df = pd.DataFrame(rows)\n","\n","    # enforce column order where present\n","    df = df[[c for c in FIELDNAMES if c in df.columns]]\n","\n","    before = len(df)\n","    df = df.drop_duplicates(subset=\"permalink\", keep=\"first\")\n","    after = len(df)\n","    print(f\"üßπ Deduplicated by permalink: {before} ‚Üí {after}\")\n","\n","    df.to_csv(out_path, index=False)\n","    print(f\"üìÅ Saved {len(df)} rows to '{out_path}' (no index).\")\n","    return True\n","\n","# ---------------- main usage example ----------------\n","SUBS = [\"education\", \"teachers\", \"college\"]\n","\n","hot_rows = fetch_hot_posts(SUBS, limit=50)\n","search_rows = search_posts(\"homework\", SUBS, limit=25)\n","all_rows = hot_rows + search_rows\n","export_posts_to_csv(all_rows, out_path=\"reddit_data.csv\")\n","\n","print(\"üéâ Data collection completed successfully!\")"]}]}