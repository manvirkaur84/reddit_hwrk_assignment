{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reddit Data Collector\n",
        "This notebook pulls posts from Reddit using PRAW, cleans them, and exports to CSV.\n",
        "Author: Manvir Kaur\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "FIELDNAMES = [\n",
        "    \"title\",          # String\n",
        "    \"score\",          # Integer\n",
        "    \"upvote_ratio\",   # Float\n",
        "    \"num_comments\",   # Integer\n",
        "    \"author\",         # String\n",
        "    \"subreddit\",      # String\n",
        "    \"url\",            # String\n",
        "    \"permalink\",      # String\n",
        "    \"created_utc\",    # Integer\n",
        "    \"is_self\",        # Boolean\n",
        "    \"selftext\",       # String (truncated to 500)\n",
        "    \"flair\",          # String\n",
        "    \"domain\",         # String\n",
        "    \"search_query\",   # String (None if not using search)\n",
        "]\n",
        "\n",
        "def _to_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _row_from_post(post, subreddit_name: str, search_query: str | None) -> Dict[str, Any]:\n",
        "    author = post.author.name if post.author else None\n",
        "    text = post.selftext if isinstance(post.selftext, str) else None\n",
        "    if text and len(text) > 500:\n",
        "        text = text[:500] + \"...\"\n",
        "    return {\n",
        "        \"title\": getattr(post, \"title\", None),\n",
        "        \"score\": getattr(post, \"score\", None),\n",
        "        \"upvote_ratio\": getattr(post, \"upvote_ratio\", None),\n",
        "        \"num_comments\": getattr(post, \"num_comments\", None),\n",
        "        \"author\": author,\n",
        "        \"subreddit\": subreddit_name,\n",
        "        \"url\": getattr(post, \"url\", None),\n",
        "        \"permalink\": (\"https://reddit.com\" + getattr(post, \"permalink\",\"\")) if getattr(post,\"permalink\",None) else None,\n",
        "        \"created_utc\": _to_int(getattr(post, \"created_utc\", None)),\n",
        "        \"is_self\": getattr(post, \"is_self\", None),\n",
        "        \"selftext\": text,\n",
        "        \"flair\": getattr(post, \"link_flair_text\", None),\n",
        "        \"domain\": getattr(post, \"domain\", None),\n",
        "        \"search_query\": search_query if search_query else None,\n",
        "    }\n",
        "\n",
        "def _normalize_subs(subreddit_name) -> List[str]:\n",
        "    if isinstance(subreddit_name, str):\n",
        "        subs = [subreddit_name]\n",
        "    else:\n",
        "        subs = subreddit_name\n",
        "    return [s.strip().lstrip(\"r/\") for s in subs if isinstance(s, str) and s.strip()]\n",
        "\n",
        "# ---- Task 1: HOT posts\n",
        "def fetch_hot_posts(subreddit_name, limit=50) -> List[Dict[str,Any]]:\n",
        "    subs = _normalize_subs(subreddit_name)\n",
        "    rows: List[Dict[str,Any]] = []\n",
        "    for sub in subs:\n",
        "        print(f\"\u2b07\ufe0f  r/{sub}: collecting HOT (limit={limit}) ...\")\n",
        "        count = 0\n",
        "        for post in reddit.subreddit(sub).hot(limit=limit):\n",
        "            rows.append(_row_from_post(post, sub, search_query=None))\n",
        "            count += 1\n",
        "        print(f\"\u2705 Collected {count} hot posts from r/{sub}.\")\n",
        "    return rows\n",
        "\n",
        "# ---- Task 2: keyword-based search\n",
        "def search_posts(query: str, subreddit_name, limit=50) -> List[Dict[str,Any]]:\n",
        "    subs = _normalize_subs(subreddit_name)\n",
        "    rows: List[Dict[str,Any]] = []\n",
        "    for sub in subs:\n",
        "        print(f\"\ud83d\udd0e r/{sub}: searching '{query}' (limit={limit}) ...\")\n",
        "        count = 0\n",
        "        for post in reddit.subreddit(sub).search(query=query, sort=\"new\", limit=limit):\n",
        "            rows.append(_row_from_post(post, sub, search_query=query))\n",
        "            count += 1\n",
        "        print(f\"\u2705 Collected {count} search posts from r/{sub}.\")\n",
        "    return rows\n",
        "\n",
        "# ---- Task 3: data export to CSV\n",
        "def export_posts_to_csv(rows: List[Dict[str,Any]], out_path: str = \"reddit_data.csv\") -> bool:\n",
        "    if not rows:\n",
        "        print(\"\u26a0\ufe0f No rows to export.\")\n",
        "        return False\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # enforce column order where present\n",
        "    df = df[[c for c in FIELDNAMES if c in df.columns]]\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=\"permalink\", keep=\"first\")\n",
        "    after = len(df)\n",
        "    print(f\"\ud83e\uddf9 Deduplicated by permalink: {before} \u2192 {after}\")\n",
        "\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"\ud83d\udcc1 Saved {len(df)} rows to '{out_path}' (no index).\")\n",
        "    return True\n",
        "\n",
        "# ---------------- main usage example ----------------\n",
        "SUBS = [\"education\", \"teachers\", \"college\"]\n",
        "\n",
        "hot_rows = fetch_hot_posts(SUBS, limit=50)\n",
        "search_rows = search_posts(\"homework\", SUBS, limit=25)\n",
        "all_rows = hot_rows + search_rows\n",
        "export_posts_to_csv(all_rows, out_path=\"reddit_data.csv\")\n",
        "\n",
        "print(\"\ud83c\udf89 Data collection completed successfully!\")\n"
      ]
    }
  ]
}